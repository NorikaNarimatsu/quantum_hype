{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c2ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa29d6e",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9caa7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_excel_sheet(excel_files, sheet_name, output_folder, csv_filename):\n",
    "    \"\"\"\n",
    "    Extract a specific sheet from multiple Excel files, combine them into one DataFrame, and save as CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - excel_files (list): List of Excel file paths.\n",
    "    - sheet_name (str): Name of the sheet to extract (must exist in the file).\n",
    "    - output_folder (str): Folder where CSV will be saved.\n",
    "    - csv_filename (str): Name of the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The combined DataFrame.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    all_dfs = []\n",
    "\n",
    "    for file in excel_files:\n",
    "        xls = pd.ExcelFile(file)\n",
    "        if sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            df['Source_File'] = os.path.basename(file)\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Sheet '{sheet_name}' not found in {file}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\" No data extracted.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    csv_path = os.path.join(output_folder, csv_filename)\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {len(combined_df)} rows from sheet '{sheet_name}' to {csv_path}\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92222f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_files = [\n",
    "    '../data/raw/2016_12_31.xlsx',\n",
    "    '../data/raw/2017_01_2021_03.xlsx',\n",
    "    '../data/raw/2021_03_2023_12.xlsx'\n",
    "]\n",
    "output_folder = '../data/csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d0613",
   "metadata": {},
   "source": [
    "### Concacanate Excel files for output and author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b7bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28283 rows from sheet 'ResearchOutput' to ../data/csv/df_output_combined.csv\n",
      "Saved 194724 rows from sheet 'Addresses and Names' to ../data/csv/df_author_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load df_output\n",
    "df_output = extract_excel_sheet(\n",
    "    excel_files=excel_files,\n",
    "    sheet_name='ResearchOutput',\n",
    "    output_folder=output_folder,\n",
    "    csv_filename='df_output_combined.csv'\n",
    ")\n",
    "\n",
    "# Step 2: Load df_author\n",
    "df_author = extract_excel_sheet(\n",
    "    excel_files=excel_files,\n",
    "    sheet_name='Addresses and Names',\n",
    "    output_folder=output_folder,\n",
    "    csv_filename='df_author_combined.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0646218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_output columns: ['paperId', 'docType', 'keywords', 'keywordsPlus', 'subjectSub1', 'subjectCat1', 'subjectCatExt', 'categoryHeading', 'abstract', 'year', 'doi', 'sourceTitle', 'title']\n",
      "df_author columns: ['paperId', 'country', 'location', 'orgNames', 'orgEnhanced1', 'orgEnhancedAll', 'fullName', 'researcherId', 'authorSeq']\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.read_csv(\"../data/csv/df_output_combined.csv\")\n",
    "df_author = pd.read_csv(\"../data/csv/df_author_combined.csv\")\n",
    "\n",
    "columns_to_keep = {\n",
    "    'Accession Number (UT)': 'paperId',\n",
    "    '1st Document Type': 'docType',\n",
    "    'Keywords': 'keywords',\n",
    "    'Keywords Plus': 'keywordsPlus',\n",
    "    '1st Subject Sub-Heading': 'subjectSub1',\n",
    "    '1st Subject Category (traditional)': 'subjectCat1',\n",
    "    'Subject Category (extended)': 'subjectCatExt',\n",
    "    '1st Category Heading': 'categoryHeading',\n",
    "    'Abstract': 'abstract',\n",
    "    'Published Year': 'year',\n",
    "    'DOI': 'doi',\n",
    "    'Source title': 'sourceTitle',\n",
    "    'Title': 'title'\n",
    "}\n",
    "\n",
    "columns_to_keep_authors = {\n",
    "    'Accession Number (UT)': 'paperId',\n",
    "    'Country': 'country',\n",
    "    'Location': 'location',\n",
    "    'Organisation names (concatenated)': 'orgNames',\n",
    "    '1st Enhanced Organisation name': 'orgEnhanced1',\n",
    "    'Enhanced Organisation Names (concatenated)': 'orgEnhancedAll',\n",
    "    'Full Name': 'fullName',\n",
    "    'ResearcherID': 'researcherId',\n",
    "    'Researcher/Author SeqNo (position)': 'authorSeq'\n",
    "}\n",
    "\n",
    "existing_cols = [col for col in columns_to_keep if col in df_output.columns]\n",
    "df_output = df_output[existing_cols].rename(columns={col: columns_to_keep[col] for col in existing_cols})\n",
    "\n",
    "existing_cols_authors = [col for col in columns_to_keep_authors if col in df_author.columns]\n",
    "df_author = df_author[existing_cols_authors].rename(\n",
    "    columns={col: columns_to_keep_authors[col] for col in existing_cols_authors})\n",
    "\n",
    "print(\"df_output columns:\", df_output.columns.tolist())\n",
    "print(\"df_author columns:\", df_author.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8e0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered df_output: 28283 → 28076 (2007–2023)\n",
      "Filtered df_author: 194724 → 193376 (matching paperIds from df_output_cleaned)\n",
      "\n",
      "Unique paperIds in df_output_cleaned: 28076\n",
      "Unique paperIds in df_author_cleaned: 28033\n",
      "PaperIds in df_output_cleaned but missing in df_author_cleaned: 43\n",
      "\n",
      "Example paperIds missing in df_author_cleaned:\n",
      "['WOS:000259566800001', 'WOS:000214815000012', 'WOS:000304507700006', 'WOS:000251053800010', 'WOS:000258760400001', 'WOS:000437852100006', 'WOS:000414069200006', 'WOS:000284196500002', 'WOS:000264397200009', 'WOS:000482989700061']\n",
      "Removed 51543 duplicate rows based on (paperId, country, researcherId).\n",
      "Remaining rows: 141833\n",
      "Missing authorSeq values before fill: 1850\n",
      "authorSeq column cleaned. Now using -1 for missing values.\n",
      "\n",
      "After removing mismatched paperIds:\n",
      "Unique paperIds in df_output_cleaned: 28033\n",
      "Unique paperIds in df_author_cleaned: 28033\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter df_output by publication year (2007–2023)\n",
    "before_filter_output = len(df_output)\n",
    "df_output['year'] = pd.to_numeric(df_output['year'], errors='coerce')  # Ensure year is numeric\n",
    "df_output_cleaned = df_output[df_output['year'].between(2007, 2023, inclusive='both')]  # Keep only valid years\n",
    "after_filter_output = len(df_output_cleaned)\n",
    "print(f\"Filtered df_output: {before_filter_output} → {after_filter_output} (2007–2023)\")\n",
    "\n",
    "# Step 2: Filter df_author to keep only entries whose paperId exists in the cleaned df_output\n",
    "valid_paper_ids = set(df_output_cleaned['paperId'].dropna().unique())  # Collect valid paperIds\n",
    "before_filter_author = len(df_author)\n",
    "df_author_cleaned = df_author[df_author['paperId'].isin(valid_paper_ids)]  # Filter author data accordingly\n",
    "after_filter_author = len(df_author_cleaned)\n",
    "print(f\"Filtered df_author: {before_filter_author} → {after_filter_author} (matching paperIds from df_output_cleaned)\")\n",
    "\n",
    "# Step 3: Identify and report paperIds that are in df_output_cleaned but have no matching entries in df_author\n",
    "paper_ids_output = set(df_output_cleaned['paperId'].dropna().unique())\n",
    "paper_ids_author = set(df_author_cleaned['paperId'].dropna().unique())\n",
    "missing_in_author = paper_ids_output - paper_ids_author  # Difference = missing links\n",
    "\n",
    "print(f\"\\nUnique paperIds in df_output_cleaned: {len(paper_ids_output)}\")\n",
    "print(f\"Unique paperIds in df_author_cleaned: {len(paper_ids_author)}\")\n",
    "print(f\"PaperIds in df_output_cleaned but missing in df_author_cleaned: {len(missing_in_author)}\")\n",
    "\n",
    "# Optionally show a few of the missing ones\n",
    "print(\"\\nExample paperIds missing in df_author_cleaned:\")\n",
    "print(list(missing_in_author)[:10])\n",
    "\n",
    "# Step 4: Remove those unmatched paperIds from both DataFrames to keep everything consistent\n",
    "df_output_cleaned = df_output_cleaned[df_output_cleaned['paperId'].isin(paper_ids_author)]\n",
    "df_author_cleaned = df_author_cleaned[df_author_cleaned['paperId'].isin(paper_ids_author)]\n",
    "\n",
    "# Step 5: Drop exact duplicate author rows — we consider duplicates to be identical (paperId, country, researcherId)\n",
    "before_dedup = len(df_author_cleaned)\n",
    "df_author_cleaned = df_author_cleaned.drop_duplicates(\n",
    "    subset=['paperId', 'country', 'researcherId'],\n",
    "    keep='first'  # Keep the first occurrence\n",
    ")\n",
    "after_dedup = len(df_author_cleaned)\n",
    "print(f\"Removed {before_dedup - after_dedup} duplicate rows based on (paperId, country, researcherId).\")\n",
    "print(f\"Remaining rows: {after_dedup}\")\n",
    "\n",
    "# Step 6: Clean authorSeq column — convert to numeric and fill missing values with -1\n",
    "df_author_cleaned['authorSeq'] = pd.to_numeric(df_author_cleaned['authorSeq'], errors='coerce')  # Convert to numeric\n",
    "missing_count = df_author_cleaned['authorSeq'].isna().sum()\n",
    "print(f\"Missing authorSeq values before fill: {missing_count}\")\n",
    "df_author_cleaned['authorSeq'] = df_author_cleaned['authorSeq'].fillna(-1).astype(int)  # Fill with -1 and convert to int\n",
    "print(f\"authorSeq column cleaned. Now using -1 for missing values.\")\n",
    "\n",
    "# Debug\n",
    "df_author_cleaned.loc[\n",
    "    (df_author_cleaned['fullName'] == 'Han, Rui') &\n",
    "    (df_author_cleaned['paperId'] == 'WOS:000372420000001'),\n",
    "    'researcherId'\n",
    "] = \"customID\"\n",
    "\n",
    "df_author_cleaned.loc[\n",
    "    (df_author_cleaned['fullName'] == 'Guo, Yanshu') &\n",
    "    (df_author_cleaned['paperId'] == 'WOS:001089303900010'),\n",
    "    'researcherId'\n",
    "] = \"customID2\"\n",
    "\n",
    "# Final confirmation: show how many unique paperIds are in both cleaned DataFrames\n",
    "print(f\"\\nAfter removing mismatched paperIds:\")\n",
    "print(f\"Unique paperIds in df_output_cleaned: {df_output_cleaned['paperId'].nunique()}\")\n",
    "print(f\"Unique paperIds in df_author_cleaned: {df_author_cleaned['paperId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b4f8422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Strategy 2: Added 24896 rows from unique authorSeq==1\n",
      "Shape: (24896, 10)\n",
      "\n",
      "✔ Strategy 3: Added 890 fallback rows (solo authors, authorSeq == -1)\n",
      "Shape: (25786, 10)\n",
      "\n",
      "✔ Strategy 4: Merged and added 1827 rows (same researcherId, multiple affiliations)\n",
      "Shape: (27613, 10)\n",
      "\n",
      "✔ Strategy 5: Merged and added 387 fallback rows (multi-author -1, no researcherId)\n",
      "Shape: (387, 10)\n",
      "\n",
      "✔ Strategy 6: Added 13 rows (only one author, not seq==1)\n",
      "Shape: (28013, 10)\n",
      "\n",
      "✔ Strategy 7: Added 20 rows (min authorSeq with merge if duplicate)\n",
      "Final shape after Strategy 7: (28033, 10)\n",
      "Unique paperIds in original: 28033\n",
      "Unique paperIds in final: 28033\n"
     ]
    }
   ],
   "source": [
    "# Start fresh: initialize empty DataFrame for first-author selections\n",
    "df_author_first_combined = pd.DataFrame(columns=df_author_cleaned.columns.tolist() + ['firstAuthorSource'])\n",
    "\n",
    "# =======================================\n",
    "# Strategy 1: duplicated row is done in previous codeblock\n",
    "# =======================================\n",
    "\n",
    "# =======================================\n",
    "# Strategy 2: Unique authorSeq == 1\n",
    "# =======================================\n",
    "\n",
    "# Step 2.1: Filter candidates\n",
    "df_first_author_candidates = df_author_cleaned[df_author_cleaned['authorSeq'] == 1]\n",
    "\n",
    "# Step 2.2: Keep only paperIds with exactly one authorSeq==1\n",
    "seq1_counts = df_first_author_candidates['paperId'].value_counts()\n",
    "unique_seq1_paperIds = seq1_counts[seq1_counts == 1].index\n",
    "\n",
    "# Step 2.3: Select rows\n",
    "df_seq1_first = df_first_author_candidates[df_first_author_candidates['paperId'].isin(unique_seq1_paperIds)].copy()\n",
    "df_seq1_first['firstAuthorSource'] = 'seq1_unique'\n",
    "\n",
    "# Step 2.4: Add to combined\n",
    "df_author_first_combined = pd.concat([df_author_first_combined, df_seq1_first], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✔ Strategy 2: Added {df_seq1_first.shape[0]} rows from unique authorSeq==1\")\n",
    "print(f\"Shape: {df_author_first_combined.shape}\")\n",
    "\n",
    "# =======================================\n",
    "# Strategy 3: Solo author, authorSeq == -1\n",
    "# =======================================\n",
    "\n",
    "# Step 3.1: Identify uncovered papers\n",
    "covered_paper_ids = set(df_author_first_combined['paperId'])\n",
    "df_author_remaining = df_author_cleaned[~df_author_cleaned['paperId'].isin(covered_paper_ids)]\n",
    "\n",
    "# Step 3.2: Find solo-author papers\n",
    "author_counts = df_author_cleaned['paperId'].value_counts()\n",
    "solo_papers = author_counts[author_counts == 1].index\n",
    "\n",
    "# Step 3.3: Select those with authorSeq == -1\n",
    "df_fallback_first = df_author_remaining[\n",
    "    (df_author_remaining['paperId'].isin(solo_papers)) &\n",
    "    (df_author_remaining['authorSeq'] == -1)\n",
    "].copy()\n",
    "df_fallback_first['firstAuthorSource'] = 'fallback_-1_single'\n",
    "\n",
    "# Step 3.4: Add to combined\n",
    "df_author_first_combined = pd.concat([df_author_first_combined, df_fallback_first], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✔ Strategy 3: Added {df_fallback_first.shape[0]} fallback rows (solo authors, authorSeq == -1)\")\n",
    "print(f\"Shape: {df_author_first_combined.shape}\")\n",
    "\n",
    "# =======================================\n",
    "# Strategy 4: Same researcherId, multiple rows with authorSeq == 1\n",
    "# =======================================\n",
    "\n",
    "# Step 4.1: Filter uncovered + authorSeq == 1\n",
    "covered_paper_ids = set(df_author_first_combined['paperId'])\n",
    "df_author_remaining = df_author_cleaned[~df_author_cleaned['paperId'].isin(covered_paper_ids)]\n",
    "df_seq1_remaining = df_author_remaining[df_author_remaining['authorSeq'] == 1]\n",
    "\n",
    "# Step 4.2: Find paperId + researcherId combos with >1 row\n",
    "duplicate_seq1_ids = (\n",
    "    df_seq1_remaining.groupby(['paperId', 'researcherId'])\n",
    "    .size().reset_index(name='count')\n",
    "    .query('count > 1')\n",
    ")\n",
    "\n",
    "# Step 4.3: Merge rows for these combos\n",
    "merged_rows = []\n",
    "for _, row in duplicate_seq1_ids.iterrows():\n",
    "    paper_id = row['paperId']\n",
    "    researcher_id = row['researcherId']\n",
    "    subset = df_seq1_remaining[\n",
    "        (df_seq1_remaining['paperId'] == paper_id) &\n",
    "        (df_seq1_remaining['researcherId'] == researcher_id)\n",
    "    ]\n",
    "    merged = subset.iloc[0].copy()\n",
    "    merged['country'] = '; '.join(sorted(set(subset['country'].dropna())))\n",
    "    merged['orgNames'] = '; '.join(sorted(set(subset['orgNames'].dropna())))\n",
    "    merged['orgEnhanced1'] = '; '.join(sorted(set(subset['orgEnhanced1'].dropna())))\n",
    "    merged['orgEnhancedAll'] = '; '.join(sorted(set(subset['orgEnhancedAll'].dropna())))\n",
    "    merged['firstAuthorSource'] = 'merged_seq1_same_id'\n",
    "    merged_rows.append(merged)\n",
    "\n",
    "df_merged_seq1 = pd.DataFrame(merged_rows)\n",
    "df_author_first_combined = pd.concat([df_author_first_combined, df_merged_seq1], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✔ Strategy 4: Merged and added {df_merged_seq1.shape[0]} rows (same researcherId, multiple affiliations)\")\n",
    "print(f\"Shape: {df_author_first_combined.shape}\")\n",
    "\n",
    "# =======================================\n",
    "# Strategy 5: Multi-author, all -1, no researcherId\n",
    "# =======================================\n",
    "\n",
    "# Step 5.1: Identify remaining uncovered papers\n",
    "covered_paper_ids = set(df_author_first_combined['paperId'])\n",
    "uncovered_paper_ids = set(df_author_cleaned['paperId']) - covered_paper_ids\n",
    "df_remaining = df_author_cleaned[df_author_cleaned['paperId'].isin(uncovered_paper_ids)]\n",
    "\n",
    "# Step 5.2: Filter where authorSeq == -1 and no researcherId\n",
    "df_no_id = df_remaining[\n",
    "    (df_remaining['authorSeq'] == -1) &\n",
    "    (df_remaining['researcherId'].isna())\n",
    "]\n",
    "\n",
    "# Step 5.3: Group by paperId and merge metadata\n",
    "grouped_rows = []\n",
    "for paper_id, group in df_no_id.groupby('paperId'):\n",
    "    merged = group.iloc[0].copy()\n",
    "    merged['country'] = '; '.join(sorted(set(group['country'].dropna())))\n",
    "    merged['orgNames'] = '; '.join(sorted(set(group['orgNames'].dropna())))\n",
    "    merged['orgEnhanced1'] = '; '.join(sorted(set(group['orgEnhanced1'].dropna())))\n",
    "    merged['orgEnhancedAll'] = '; '.join(sorted(set(group['orgEnhancedAll'].dropna())))\n",
    "    merged['firstAuthorSource'] = 'merged_-1_no_id'\n",
    "    grouped_rows.append(merged)\n",
    "\n",
    "df_merged_fallback = pd.DataFrame(grouped_rows)\n",
    "df_author_first_combined = pd.concat([df_author_first_combined, df_merged_fallback], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✔ Strategy 5: Merged and added {df_merged_fallback.shape[0]} fallback rows (multi-author -1, no researcherId)\")\n",
    "print(f\"Shape: {df_merged_fallback.shape}\")\n",
    "\n",
    "# =======================================\n",
    "# Strategy 6: Only one author, but not seq==1 (e.g. seq=-1)\n",
    "# =======================================\n",
    "\n",
    "# Step 6.1: Identify remaining uncovered paperIds\n",
    "covered_paper_ids = set(df_author_first_combined['paperId'])\n",
    "uncovered_paper_ids = set(df_author_cleaned['paperId']) - covered_paper_ids\n",
    "df_remaining = df_author_cleaned[df_author_cleaned['paperId'].isin(uncovered_paper_ids)]\n",
    "\n",
    "# Step 6.2: Count number of authors per paper\n",
    "remaining_author_counts = df_remaining['paperId'].value_counts()\n",
    "unique_author_paperIds = remaining_author_counts[remaining_author_counts == 1].index\n",
    "\n",
    "# Step 6.3: Select the single remaining author rows\n",
    "df_unique_author_remaining = df_remaining[df_remaining['paperId'].isin(unique_author_paperIds)].copy()\n",
    "df_unique_author_remaining['firstAuthorSource'] = 'only_one_author_non_seq1'\n",
    "\n",
    "# Step 6.4: Add to combined\n",
    "df_author_first_combined = pd.concat([df_author_first_combined, df_unique_author_remaining], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✔ Strategy 6: Added {df_unique_author_remaining.shape[0]} rows (only one author, not seq==1)\")\n",
    "print(f\"Shape: {df_author_first_combined.shape}\")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# Strategy 7: No authorSeq == 1 → pick min authorSeq row (combine if tied)\n",
    "# =======================================\n",
    "\n",
    "# Step 7.1: Remaining uncovered paperIds\n",
    "covered_paper_ids = set(df_author_first_combined['paperId'])\n",
    "all_paper_ids = set(df_author_cleaned['paperId'])\n",
    "uncovered_paper_ids = all_paper_ids - covered_paper_ids\n",
    "\n",
    "# Step 7.2: Subset with those papers\n",
    "df_uncovered = df_author_cleaned[df_author_cleaned['paperId'].isin(uncovered_paper_ids)]\n",
    "\n",
    "# Step 7.3: Get paperIds that have no authorSeq == 1\n",
    "paper_ids_no_seq1 = (\n",
    "    df_uncovered.groupby('paperId')\n",
    "    .filter(lambda g: (g['authorSeq'] != 1).all())\n",
    "    .paperId.unique()\n",
    ")\n",
    "\n",
    "df_no_seq1 = df_uncovered[df_uncovered['paperId'].isin(paper_ids_no_seq1)].copy()\n",
    "\n",
    "# Step 7.4: Process each paperId\n",
    "merged_rows = []\n",
    "for paper_id, group in df_no_seq1.groupby('paperId'):\n",
    "    min_seq = group['authorSeq'].min()\n",
    "    subset = group[group['authorSeq'] == min_seq]\n",
    "\n",
    "    if len(subset) == 1:\n",
    "        row = subset.iloc[0].copy()\n",
    "        row['firstAuthorSource'] = 'strategy7_min_seq_single'\n",
    "        merged_rows.append(row)\n",
    "    else:\n",
    "        merged = subset.iloc[0].copy()\n",
    "        merged['country'] = '; '.join(sorted(set(subset['country'].dropna())))\n",
    "        merged['orgNames'] = '; '.join(sorted(set(subset['orgNames'].dropna())))\n",
    "        merged['orgEnhanced1'] = '; '.join(sorted(set(subset['orgEnhanced1'].dropna())))\n",
    "        merged['orgEnhancedAll'] = '; '.join(sorted(set(subset['orgEnhancedAll'].dropna())))\n",
    "        merged['firstAuthorSource'] = 'strategy7_min_seq_merged'\n",
    "        merged_rows.append(merged)\n",
    "\n",
    "df_strategy7 = pd.DataFrame(merged_rows)\n",
    "\n",
    "# Step 7.5: Append to combined\n",
    "df_author_first_combined = pd.concat([df_author_first_combined, df_strategy7], ignore_index=True)\n",
    "\n",
    "# Final Check\n",
    "print(f\"\\n✔ Strategy 7: Added {df_strategy7.shape[0]} rows (min authorSeq with merge if duplicate)\")\n",
    "print(f\"Final shape after Strategy 7: {df_author_first_combined.shape}\")\n",
    "print(f\"Unique paperIds in original: {df_author_cleaned['paperId'].nunique()}\")\n",
    "print(f\"Unique paperIds in final: {df_author_first_combined['paperId'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0201ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28033, 10)\n",
      "(28033, 13)\n",
      "Merged shape: (28033, 22)\n",
      "Saved df_combined_full in csv folder\n"
     ]
    }
   ],
   "source": [
    "print(df_author_first_combined.shape)\n",
    "print(df_output_cleaned.shape)\n",
    "\n",
    "# Merge based on paperId\n",
    "df_combined_full = df_output_cleaned.merge(\n",
    "    df_author_first_combined,\n",
    "    on='paperId',\n",
    "    how='left',  # Keeps all rows from df_output_cleaned\n",
    "    suffixes=('', '_author')  # Prevents name collision\n",
    ")\n",
    "\n",
    "print(f\"Merged shape: {df_combined_full.shape}\")\n",
    "\n",
    "csv_path = os.path.join(output_folder, \"df_final.csv\")\n",
    "df_combined_full.to_csv(csv_path, index=False)\n",
    "print(\"Saved df_combined_full in csv folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26edec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_author' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_author\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_author' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
